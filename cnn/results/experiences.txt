RUN 0:
- Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7

RUN 1 et 2: Mauvaise manip avec d laissé fixé à 7. Les runs répliquent donc les résultats du RUN 0.

---------------------------------------------------------------------------------------------------------
A) INFLUENCE DU PARAMETRE d
---------------------------------------------------------------------------------------------------------

RUN 3:
- Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=4

RUN 4:
- Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=5

RUN 5:
- Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=6

CONCLUSION des run 3 à 5: Il est préférable de laisser d=7. Entre d=6 et d=7, l'amélioration semble
relativement faible.
bon intermédiaire entre temps de calcul et performances

RUN 11: EN COURS
- Learning rate initialisé à 0.01, fixé à 0.001 après 10 époques
- Pas de régularisation TV
- Profondeur du réseau fixée à d=8 

---------------------------------------------------------------------------------------------------------
B) INFLUENCE DU LEARNING RATE
---------------------------------------------------------------------------------------------------------

une courbe pour chaque run, validation loss

RUN 6:
- Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7
- Initialisation à partir des poids de l'époque 5 du RUN0. L'objectif est de voir si on 
ne décroît pas trop vite le learning rate en début de calcul. 
CONCLUSION: learning rate de 0.01 trop élevé

RUN 7:
- Learning rate fixé à 0.001
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7
- Initialisation à partir des poids de l'époque 40 du RUN 0. 
CONCLUSION: les erreurs d'entraînement et de validation continuent de décroître 


RUN 8:
- Learning rate fixé à 0.01
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7
- Initialisation à partir des poids de l'époque 40 du RUN 0. L'objectif est de voir si on 
ne décroît pas trop vite le learning rate en début de calcul.
CONCLUSION: On diverge

RUN 9:
- Learning rate initialisé à 0.001, divisé par 2 toutes les 2 époques, saturation à 1e-4
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7
- Initialisation à partir des poids de l'époque 40 du RUN 0. L'objectif est de voir si on 
ne décroît pas trop vite le learning rate en début de calcul.
CONCLUSION: les erreurs d'entraînement et de validation décroissent concommitamment

RUN 10:
- Learning rate fixé à 0.001
- Pas de régularisation TV
- Profondeur du réseau fixée à d=7
- Initialisation à partir des poids de l'époque 40 du RUN 9.

CONCLUSIONS: Il semble pertinent de partir d'un learning rate de 0.01 puis de décroître le 
learning rate à 0.001 après une dizaine d'époques. Au bout d'une centaine d'époque, on atteint
un palier pour l'erreur de validation, tout en continuant de réduire l'erreur d'entraînement.

Résultats pour le RUN 10 en terme de métriques superpixel:
Recall: 0.878094445585659
Undersegmentation: 0.03884787610862378
Undersegmentation (NP): 0.07706343184614214
Compactness: 0.7681572468991564

Résultat de référence:
Recall: 0.8995381636824131 (Légèrement meilleur que RUN 10), pas très grave parce que la compacité est pourrie
du coup comme les contours oscillent ils intersectent plus de contours de l'image
Undersegmentation: 0.04733669591333142 (nettement moins bon que RUN 10)
Undersegmentation (NP): 0.09382589731561694 (nettement moins bon que RUN 10)
Compactness: 0.5409056244400318 (nettement moins bon que RUN 10)

 
---------------------------------------------------------------------------------------------------------
C) VARIATION TOTALE
---------------------------------------------------------------------------------------------------------

RUN 12: EN COURS
- Learning rate initialisé à 0.01, fixé à 0.001 après 10 époques
- Régularisation TV: 1e-6
- Profondeur du réseau fixée à d=7 

RUN 13: EN COURS
- Learning rate initialisé à 0.01, fixé à 0.001 après 10 époques
- Régularisation TV: 1e-7
- Profondeur du réseau fixée à d=7 



