\documentclass{article}
\usepackage{preambule}

\begin{document}

\newcommand{\spp}{superpixel}

\begin{center}
    \begin{Large}\textbf{IMAGE SEGMENTATION BY SUPERPIXELS}\end{Large}

    \vspace{1cm}
    \begin{large}\textbf{\underline{T.Dumont$^a$}, B.Figliuzzi$^b$}\end{large}

    \vspace{0.5cm}
    a. MINES ParisTech, theo.dumont@mines-paristech.fr\\
    b. MINES ParisTech CMM, bruno.figliuzzi@mines-paristech.fr
    \vspace{1cm}
\end{center}

\begin{center}
\noindent\textbf{Key-words: }\\
deep learning; convolutional neural networks; image segmentation\\
\ \\
\textbf{Abstract: }\\
In this paper, we study different options to improve the performance of a deep learning convolutional neural network
\end{center}

\tableofcontents

\section*{Todo}
\begin{itemize}
    \item
\end{itemize}

\newpage
\section{Introduction}

    \subsection{Segmentation}
            \paragraph{What it is}piche

            \begin{figure}[!htb]
                \centering
                \includegraphics[scale=0.5]{pics/img.png}
                \caption{\textit{An image and its segmented image}}
                \label{fig:segm}
            \end{figure}
            \paragraph{Motivation}

    \subsection{Superpixels}
            \paragraph{What it is}piche

            \begin{figure}[!htb]
                \centering
                \includegraphics[scale=0.5]{pics/img.png}
                \caption{\textit{Partition d'une image en superpixels}}
                \label{fig:spp}
            \end{figure}

            \paragraph{Applications \& Motivation}
            démarrer une segmentation\\
            fournir un support sur lequel faire de la classification (couleur/texture moyenne, etc)


    \subsection{Ce qu'est une bonne \spp segmentation}
        \subsubsection{Metrics}
        cf article https://arxiv.org/pdf/1612.01601.pdf
        let $S = {S_j}^K_{j=1}$ and $G = {G_i}$ be partitions of the same image $I : x_n \mapsto I(x_n)$, $1 \leq n \leq N$
        $S$ is a segmented image
        $G$ is the ground truth
            \paragraph{Boundary Recall}
            - most commonly used metric to assess boundary adherence.
            - Let $\text{TP}(G,S)$ be the number of true positive boundary pixels and $\text{FN}(G,S)$ be the number of false negative boudary pixels in the segmented image $S$.
            $$\mathrm{Rec}(G,S)=\frac{\mathrm{TP}(G,S)}{\mathrm{TP}(G,S)+\mathrm{FN}(G,S)}$$
            \paragraph{Undersegmentation Error}

            \paragraph{Compactness}
            - evaluates the compactness of the superpixels.
            $$
            \mathrm{CO}(G, S)=\frac{1}{N} \sum_{S_{j}}|S_{j}| \frac{4 \pi A\left(S_{j}\right)}{P\left(S_{j}\right)}
            $$
            - the $\mathrm{CO}$ operator computes how close the area $A(S_j)$ of each superpixel $S_j$ is from a circle with same perimeter $P(S_j)$.
        \subsubsection{Autres algorithmes}
            \paragraph{SLIC}
            \paragraph{metrics}
            Here are the previously defined metrics of some well-known superpixel segmentation algorithms.
            \begin{table}[!ht]
                \center
                \begin{tabular}{|c|c|c|c|}
                    \hline
                    Algorithm & BR & UE & CO\\
                    \hline
                    \hline
                    SLIC & & & \\
                    \hline
                     & & & \\
                    \hline
                     & & & \\
                    \hline
                    \hline
                    Reference & & & \\
                    \hline
                \end{tabular}
                \caption{Metrics for different superpixel segmentation algorithms}
            \end{table}

            \noindent We use the ?? alorithm as a reference to evaluate the performances of our model.
    \subsection{Motivations/ambitions}
            \paragraph{Difficultés que l'on cherche à résoudre}
            \paragraph{Pas de vraie approche DL pour segmentation avec \spp s}
            \paragraph{Ambitions}
            améliorer les métriques









\section{Dataset generation}
    \subsection{COCO dataset}
        \subsubsection{The COCO dataset}
        COCO dataset\footnote{site de COCO}, nb of images, examples
        \subsubsection{Characteristics}
        \label{par:charac}

        \begin{figure}[!ht]
            \begin{subfigure}{.49\linewidth}
                \centering
                % \includegraphics[width=\linewidth]{pics/train2017full.png}
                \caption{\textit{plot}}
            \end{subfigure}
            % \hspace{-7em}
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{pics/val2017full.png}
                \caption{\textit{plot}}
            \end{subfigure}

            \bigskip

            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c|c|c||c|}
                    \hline
                     & Training & Height & Width \\
                    \hline
                    \hline
                    Images & Min & ? & ? \\
                    \hline
                    118~287 & Max & ? & ? \\
                    \hline
                \end{tabular}
                \caption{Tabular}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c||c|c|c|}
                    \hline
                     & Validation & Height & Width \\
                    \hline
                    \hline
                    Images & Min & 145 & 200 \\
                    \hline
                    5000 & Max & 640 & 640 \\
                    \hline
                \end{tabular}
                \caption{Tabular}
            \end{subfigure}
            \caption{\textit{Training and validation sets characterization}}
        \end{figure}

    \subsection{Eikonal}
    \subsection{Notre utilisation de eikonal}
    en plus réutilisé derrière sur image qui sort du réseau\\
    faire un petit résumé









\section{The model}
    \subsection{Approach}
    description générale de l'approche (NN puis eikonal)
    \subsection{Network architecture}
        \subsubsection{Layers definitions}

            \paragraph{Dilated convolution}\label{par:dilated} We consider a layer $L=(L_j)_{j\in [\![1,w]\!]}$, $w$ being the number of feature maps $L_j$ of $L$. We also consider $K=(K_{i,j})_{i,j}$, each $K_{i,j}$ being a $3\times 3$ convolutional kernel. The dilated convolution operation of $K_{i,j}$ on $L_j$ is denoted by $L_j*_r K_{i,j}$, $r$ being the dilation parameter.
            \begin{figure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-simple.png}
                    \caption{\textit{A simple convolution ($r=1$)}}
                    \label{fig:conv-simple}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-dilated.png}
                    \caption{\textit{A dilated convolution ($r=2$)}}
                    \label{fig:conv-dilated}
                \end{subfigure}
                \caption{\textit{Illustration of two types of convolutions}}
            \end{figure}
            The output $C(x)$ of a pixel $x$ is:
            \begin{flalign*}
            C(x):& = (L_j*_r K_{i,j})(x) &\\
                 & = \sum_{a+rb=x}L_j(a)K_{i,j}(b) &\\
                 & = \sum_b L_j(x-rb)K_{i,j}(b) &
            \end{flalign*}
            and we recognize the simple convolution when $r=1$.\\
            A dilated convolution enables the network getting larger receptive fields while preserving the input resolution\footnote{ref ?}


            \paragraph{Adaptative Batch Normalization (ABN)} As we have seen in (\ref{par:charac}), page \pageref{par:charac}, we need to normalize the data. We define the \textit{adaptative normalization function} $\Psi$ as:
            $$\Psi(x)=a\ x+b\ BN(x),$$
            where $BN$ is the classic batch normalization\footnote{reference ?}, defined as:
            $$BN(x) = \frac{x-\mathrm{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta.$$
            As such, $\Psi$ combines identity mapping and batch normalization. $a$, $b$, $\gamma$ and $\beta$ are learned parameters\footnote{ref : \url{https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html}} by backpropagation. It allows the model to adapt to each dataset, choosing whether or not giving a big importance to the identity term and the normalization term.

            \paragraph{Leaky rectifier (LReLU)} In order to let our neural network model complex patterns in the data, we have to add a non-linear property to the model. It often is an activation function, such as a sigmoid or a tanh (Figure \ref{fig:act-sigmoids}).

            \begin{figure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=\linewidth]{pics/act-sigmoid.png}
                    \caption{\textit{Sigmoid}}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=\linewidth]{pics/act-tanh.png}
                    \caption{\textit{tanh}}
                \end{subfigure}
                \caption{\textit{Illustration of two bounded rectifiers}}
                \label{fig:act-sigmoids}
            \end{figure}

            The problem with these activation functions is that they are bounded and their gradient is very low on the edges. Because we want are going to manipulate high scalar values, we have to use an unbounded activation function, such as ReLU, $\Phi(x)=\max(0,x)$ (Figure \ref{fig:relu}). But the issue with ReLU is that all the negative values become zero immediately, which decreases the ability of our model to train from the data. Hence the implementation of a \textit{leaky rectifier}, LReLU:
            $$\Phi(x)=\max(\alpha x,x)\mbox{, with } 0<\alpha<1.$$

            \begin{figure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/act-relu.png}
                    \caption{\textit{ReLU}}
                    \label{fig:relu}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/act-lrelu.png}
                    \caption{\textit{LReLU ($\alpha=0.2$)}}
                    \label{fig:lrelu}
                \end{subfigure}
                \caption{\textit{Illustration of two unbounded rectifiers}}
            \end{figure}

            By implementing a Leaky Rectifier, we are able to take into account the negative valued pixels.



        \subsubsection{Chen}
            \paragraph{Context Aggregation Network (CAN)}\footnote{reference}
            \begin{table}[!ht]
                \center
                \begin{tabular}{ccccccccccc}
                    \hline
                    input $I$ & $\longrightarrow$ & $L^1$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & $L^s$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & output ($L^d$)\\
                    $m\times n\times 3$ & & $m\times n\times w_1$ & & & & $m\times n\times w_s$ & & & & $m\times n\times 3$\\
                    \hline
                \end{tabular}
                \caption{\textit{Layers}}
            \end{table}
            blabla sur le RGB en entrée, RGB en sortie I -> f(I)
            \paragraph{Architecture of a block}
            Each block $L_s$ is made of 3 layers:
            \begin{enumerate}
                \item \textit{A dilated convolution}, $r_s=2^s$
                \item \textit{An adaptative batch normalization}
                \item \textit{A leaky rectifier (ReLU)}
            \end{enumerate}
            so that the content of an intermediate layer $L^s$ can be computed from the content of the previous layer $L^{s-1}$:
            \begin{equation}
                L_i^s=\Phi\left(\Psi^s\left(b_i^s+\sum_jL_j^{s-1}*_{r_s}K^s_{i,j}\right)\right).
            \end{equation}
            where ... is ...

            and
            \begin{equation}
                L_j^{s-1}*_{r_s}K^s_{i,j}=\sum_{a+r_sb=x}L_j^{s-1}(a)K_{i,j}^s(b)
            \end{equation}
            because of \ref{par:dilated}, page \pageref{par:dilated}.

            \begin{table}[!ht]
                \center
                \begin{tabular}{|c|c|c|c|c|c|c|c|}
                    \hline
                    Layer & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
                    \hline \hline
                    Convolution & $3\times3$ & & & & & & \\
                    \hline
                    Dilation & 1 & & & & & & \\
                    \hline
                    Batch Normalization & Yes & & & & & & \\
                    \hline
                    LReLU & Yes & & & & & & \\
                    \hline
                \end{tabular}
                \caption{\textit{Chen}}
            \end{table}

        \subsubsection{UNet}
        \subsubsection{Chen + UNet}

    \subsection{Total Variation (TV) Loss}
        \subsubsection{MSE}
        $$L_{MSE}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2$$

        \subsubsection{TV}
            \paragraph{Formula}
            $$L_{TV}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2+\frac{1}{N}$$
            \paragraph{Why}

    \subsection{Implementation}
    The network was implemented with PyTorch\footnote{Repository can be found at \url{https://github.com/theodumont/superpixels-segmentation}.} and we used GPU acceleration [...]
    (pytorch), se renseigner (section assez courante)
    GPU acceleraation
    code sur github









\section{Expérience et résultats}
    \subsection{Hyperparameters}
    petit bilan des valeurs choisies
    évolution des paramètres a et b ?\\
        \subsubsection{Learning rate}
        \begin{table}
            \center
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                $lr_0$ & decay? & saturation? & $d$ & TV? \\
                \hline \hline
                $0.001$ & No & No & $7$ & No \\
                \hline
                $0.01$ & No & No & $7$ & No \\
                \hline
                $0.01$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                \hline
                $0.001$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                \hline
            \end{tabular}
        \caption{Runs for learning rate tuning}
        \end{table}

        \begin{figure}[!ht]
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \caption{\textit{Tuning of learning rate}}
        \end{figure}


        \paragraph{Constant value}
    entrainement (lr, alpha) -> courbes de loss, et loss qui sature (cluster) d'où changement de lr au cours des epochs
        \paragraph{Non constant value}
        \subsubsection{Network size $d$}
        - Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
        - Pas de régularisation TV
        \begin{figure}[!ht]
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c|c|c|c|c|c|}
                    \hline
                    $d$ & 4 & 5 & 6 & 7 & 8 \\
                    \hline \hline
                    loss$\times 10^3$ & $3.46$ & $3.18$ & $3.12$ & $3.13$ & ??? \\
                    \hline
                \end{tabular}
                \caption{\textit{Loss values on validation set}}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \caption{\textit{Tuning of network size}}
        \end{figure}
        CONCLUSION des run 3 à 5: Il est préférable de laisser d=7. Entre d=6 et d=7, l'amélioration semble
        relativement faible.
        bon intermédiaire entre temps de calcul et performances

        \subsubsection{Number of epochs}
    nb epochs: on le sélectionne en prenant le minimum de la validation loss
        \subsubsection{TV regularization}

        \subsubsection{Runs}
        tableaux et graphes
    \subsection{Results on dataset}
    image originale -> CNN -> résultat du filtre dans eikonal -> superpixels sans couleurs + couleur moyenne pour chaque spp de l'image originale
    cf results/images











\section{Conclusion/Discussion}
On a présenté un nouveau...\\
On a prouvé...\\
Il reste à faire...

relire tous les mails pour avoir toutes les infos sur performances etc

\section*{Special thanks}

\section*{Sources}

\noindent
{[}1]
{[}1] C. Smith, J.C. Green, Titre de l’article, Titre du journal, 10 (2009) 55-72\\
{[}2] M. Truk, C. Bidul. Titre du bouquin, John Wiley and Sons, New York, 1973\\
{[}3] P. Machin, Titre de la thèse, Thèse, Université Poitiers, 1992\\
{[}4] D. Pierre, J.-P. Paul, B. Jacques, Titre communication, in: D. Editor, G. Editeur, ( éd.), Proceedings of Conference XXX , Publisher, Paris, France, 1995, pp. 3–6

\newpage


\end{document}
