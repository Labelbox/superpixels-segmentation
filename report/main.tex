\documentclass{article}
\usepackage{preambule}

\begin{document}

\begin{center}
    \begin{Large}\textbf{IMAGE SEGMENTATION BY SUPERPIXELS}\end{Large}

    \vspace{1cm}
    \begin{large}\textbf{\underline{T.Dumont$^a$}, B.Figliuzzi$^b$}\end{large}

    \vspace{0.5cm}
    a. MINES ParisTech, theo.dumont@mines-paristech.fr\\
    b. MINES ParisTech CMM, bruno.figliuzzi@mines-paristech.fr
    \vspace{1cm}
\end{center}

\begin{center}

\noindent\textbf{Key-words: }\\
deep learning; convolutional neural networks; image segmentation\\
\ \\
\end{center}
\textbf{Abstract: }\\
In this paper, we present an algorithm based upon convolutional neural networks for generating superpixel partitions of images.
\par
By combining an algorithm that generates superpixel partitions through the resolution of the Eikonal equation and ground truth segmentations from the Microsoft Common Objects in Context (COCO) dataset, we were able to generate training examples of superpixel partitions of the images of the dataset. These training examples arise in the form of RGB image where the color is averaged over each superpixel. A convolutional network architecture is then trained on these images. A superpixel algorithm is finally applied to the output of the network to construct the seeked partition.
\par
The algorithm is evaluated on the Berkeley Segmentation Dataset 500. It yields results in terms of boundary adherence that are comparable to the ones obtained with state of the art algorithms including SLIC, while significantly improving on these algorithms in terms of compactness and undersegmentation.

\section*{To do}
\begin{itemize}
    \item undersegmentation error
    \item SLIC
    % \item COCO dataset
    \item eikonal
    % \item global approach quickly
    \item Chen
    \item Unet
    \item MSE
    \item implementation
    \item hpp
    \begin{itemize}
        \item bilan hpp choisis
        \item network size
        \item TV loss
    \end{itemize}
    \item en annexe : tableaux de tous les runs
    \item results on dataset
    \item conclusion
\end{itemize}

\section{Introduction}

    \subsection{Segmentation}
        While looking at an image, the human brain uses a lot of knowledge to understand its content. But as for a computer, an image only is a set of integer valued pixels, making it produce an interpretation is much more complex. Segmenting an image consists in transforming the image into something that is easier to analyze, and much more meaningful. By partitioning an image into multiple sets of pixels, it assigns a label to every pixel in it such that connected groups of pixels with the same label share certain characteristics. It allows one to locate the objects on an image, pointing out their boundaries.\\
        The applications of such a process are numerous: control of the an object outlines on a production line, face detection, medical imaging, pedestrian detection, video surveillance\ldots They justify our search for higher segmentation performances.

        \begin{figure}[!ht]
        \centering
        \begin{subfigure}{.49\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_segm1.jpg}
            \caption{\textit{The original image}}
        \end{subfigure}
        \begin{subfigure}{.49\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_segm2.png}
            \caption{\textit{The segmented image}}
        \end{subfigure}
            \caption{\textit{A segmentation. To each segmented region of the image was affected the mean value of its pixels. Both images are from the COCO training dataset (REF?).}}
            \label{fig:segm}
        \end{figure}

        Image segmentation is a challenging task and there is currently no comprehensive theory in this field, not least because a given segmentation is often aimed at a specific application. The methods based on gradient can be disturbed by noise or textured patterns in the image, thus producing a contour where is actually not. In addition, it is difficult to find a contour if two regions share similar colors or gray levels, or when a contour corresponds to small gradient. Moreover, the result of a segmentation depends on the applications and on the scale of the objects of interest. Even human based segmentations are characterized by a significant variability in terms of result.

    \subsection{Superpixels}
        Superpixel algorithms are a class of techniques that partition an image into several small groups of pixels that share the same properties. Such a process highly reduces the number of characteristics of an image, as each superpixel is composed of hundreds of pixels, which leads to the amount of calculation being reduced for a further processing. In addition, as the pixels of a superpixel share similar attributes, they constitute regions of an image on which it is very relevant to compute features -- mean color, mean texture.

        \begin{figure}[!ht]
        \centering
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp1.png}
        \end{subfigure}
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp2.png}
        \end{subfigure}
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp3.png}
        \end{subfigure}
            \caption{\textit{A superpixel segmentation. From left to right: the original image, the original image with its calculated superpixels outlines and the resulting superpixel segmented image. Each superpixel has only one color, the mean color of the original image over the superpixel region.}}
            \label{fig:spp}
        \end{figure}

        Thus, superpixel algorithms are often considered as a pre-processing step in a number of applications. They can be used for depth estimation~\cite{zitnick_2007} or make the objects features more understandable in object classification. Finally, computing the superpixel partition of an image can constitute a first processing step to obtain an actual segmentation for this image~\cite{fulkerson_2009}, reducing the complexity of finding the different regions of an image by grouping at first similar pixels.

        \subsubsection{A good superpixel segmentation}
            \paragraph{Metrics}
            Before even starting building one, we have to define what is a ``good'' superpixel segmentation algorithm. As it is shown in~\cite{stutz_2017}, it is a very ambiguous task as many relevant criteria exist. Several metrics, though, are commonly chosen in the literature to evaluate whether a superpixel algorithm gives good results or not.

            Let $G = \{G_i\}_i$ and $S = \{S_j\}_j$ be partitions of the same image $I : x_n \mapsto I(x_n)$, $1 \leq n \leq N$. $G$ is the ground truth segmented image\footnote{ground truth segmented or superpixel segmented?} and $S$ is the segmented image obtained from a superpixel algorithm.
            \begin{itemize}
                \item \textbf{Boundary Recall.} The first criterion we want the algorithm to respect is quite logically to detect most of the ground truth's outlines. Boundary recall measures the intersection between the outlines of the segmented image and the ones of the ground truth image. Before the computation, the segmented image outlines are dilated by a square of side 5 pixels. As such, boundary recall indicates the proportion of real boundaries being detected, with a tolerance margin of a few pixels.\\
                If $\text{D}(G,\tilde{S})$ is the number of detected boundary pixels and $\text{UD}(G,\tilde{S})$ the number of undetected boundary pixels in the segmented image $S$, then the \textit{boundary recall} is:
                $$
                \mathrm{Rec}(G,S)=\frac{\mathrm{D}(G,\tilde{S})}{\mathrm{D}(G,\tilde{S})+\mathrm{UD}(G,\tilde{S})} \in [0,1]
                $$
                $\tilde{S}$ being the segmented image with its boundaries dilated by a square of side 5 pixels. Please note that boundary recall does not measure the regularity of the boundaries at all. That means an algorithm can have a very high boundary recall while being very tortuous. This nourishes the need of a metric that quantifies the regularity of the boundaries.

                \item \textbf{Compactness.} In order to simplify the superpixel segmented image as much as possible, its superpixels need to be smooth and regular. We thus want to build a criterion that computes how close the area $A(S_j)$ of each superpixel $S_j$ is from a circle with same perimeter $P(S_j)$:
                $$
                \mathrm{Co}(G, S)=\frac{1}{N} \sum_{S_j}|S_j| \frac{4 \pi A(S_j)}{P(S_j)^2}
                $$
                As such, a high compactness tends to indicate regular and little tortuous contours.

                \item \textbf{Undersegmentation Error.} Undersegmentation Error measures the ``leakage'' of the superpixels over the ground truth:
                $$
                \mathrm{UE}(G,S) = \text{formula}
                $$
            \end{itemize}

            \paragraph{SLIC} To do

        \subsubsection{Ambitions}
            A large amount of research has been conducted on image segmentation, based upon a variety of techniques including active contours, clustering, or region splitting or merging. These methods can be roughly classified into contour based methods and region based methods~\cite{arbelaez_2011}.

            \begin{itemize}
                \item For contour based methods, contour detection algorithms are first used to obtain candidate contours, and one has to find a way to transform these into closed contours (See~\cite{ren_2005, arbelaez_2011}). A classical approach for performing contour based segmentation is for instance the watershed algorithm.

                \item For region based methods, the image is oversegmented into small regions which are then merged to obtain an actual segmentation. To that end, a classical approach consists in representing the oversegmented image by a region adjacency graph connecting nearby regions. In this case, we transform the image segmentation problem into a graph clustering problem. A vast amount of literature in the field of image processing is dedicated to algorithms based upon graph clustering. In 2000, Shi and Malik \cite{shi_2000} proposed a novel graph-theoretic criterion to measure the effectiveness of an image partition, the normalized cut, and an efficient technique for minimizing this criterion based on a generalized eigenvalue problem. This algorithm extracts global impression and obtains good results on static images and motion sequences. In 2004, Felzenszwalb and Huttenlocher \cite{felzenszwalb_2004} introduced a graph-based image segmentation method based on pairwise region comparison. In their approach, pixels are merged according to their intensity differences across boundaries and between neighboring pixels within each region. The segmentation criteria are adaptively adjusted to take into account the variability in neighbor regions. These algorithms constitute the main approaches to perform graph-based segmentation.

            \end{itemize}

            In previous work~\cite{chang_2019}, we were able to notice that the quality of the initial oversegmentation, as measured by metrics including the boundary recall, the compactness and the undersegmentation error, significantly impacts the quality of the overall segmentation. The objective of this research is therefore to develop a deep learning based algorithm to generate a superpixel partition with improved metrics.\\
            Most of the segmentation methods are based on color information of pixels in the image, not taking into account all the human knowledge that one uses to understand what he sees. Implementing this knowledge would require a lot of computational time and a huge database, which does not currently exist. A neural network overcomes this issue, modeling this knowledge from a dataset of labeled images.


\newpage
\section{Dataset generation}
    \subsection{COCO dataset}

        The training of the neural network was performed on the COCO dataset~\cite{microsoft2014}\footnote{\url{http://cocodataset.org}}. This dataset gathers images of complex everyday scenes containing common objects in their natural context and can be used for object detection, segmentation and captioning. In order to have a better understanding of this dataset, we can analyze its image (Figure~\ref{fig:train}). Hence the necessity to pre-process the dataset in order to standardize the input images' size.

        \begin{figure}[!ht]
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{pics/train2017full.png}
                \caption{\textit{COCO dataset images: widths, heights, and number of images for each dimension}}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c||c|c|c|}
                    \hline
                    \multicolumn{2}{|c|}{Training} & Height & Width \\
                    \hline
                    \hline
                    Images & Min & 51 & 59 \\
                    \hline
                    118~287 & Max & 640 & 640 \\
                    \hline
                \end{tabular}
                \caption{\textit{Characteristics of the COCO dataset}}
            \end{subfigure}
            \caption{\textit{Training set characterization}}
            \label{fig:train}
        \end{figure}
        \noindent The COCO dataset contains a huge number of images, but its segmentations are not very qualitative. A it has been labeled by hand in an approximative way, its segmented images lack quality and their boundaries are often imprecise (Figure~\ref{fig:imprecise}). We subsequently introduce the Eikonal algorithm, which constitutes an important step in the process but that will not be fully detailed in this paper.

        \begin{figure}[!ht]
            \centering
            \includegraphics[width=0.3\linewidth]{pics/img_segm_coco.png}
            \caption{\textit{Lack of precision of the COCO dataset segmentations}}
            \label{fig:imprecise}
        \end{figure}

    \subsection{Eikonal}
        To do
    \subsection{Global approach}
        \begin{figure}[!ht]
            \centering
            \includegraphics[width=\linewidth]{pics/schema-global.png}
            \caption{\textit{Global approach. The dash type indicates if the element is an image (plain border), a segmented image (dash) or a superpixel segmented image (dash and dots).}}
        \end{figure}
        \par
        We denote as $\dot S$ a segmented image with regions of \textit{the same color} $(0, 255, 255)$  and as $\bar S$ a segmented image with regions of \textit{the same label} (0, 1, 2 \dots).
        \paragraph{Training}
        The training of the convolutional neural network runs as follows:
        \begin{enumerate}
            \item from the COCO dataset, extract the semantic segmented image $\dot C_0$ and transform it into a superpixel segmented image $\bar C_1$ with the Eikonal algorithm;
            \item from $\bar C_1$, compute the $\dot C_f$ image by assigning to each superpixel the mean color of its pixel on the original image $C_0$. The idea is that such a transformed image, relatively close to the original image, seems easier to learn by a network than a labeled image such as $\bar C_1$;
            \item use $(G,S)=(C_0, \dot C_f)$ as an training input to the neural network.
        \end{enumerate}
        \paragraph{Test}
        To test the network on the BDS dataset (\textit{cf.} Section~\ref{par:bsd}, page~\pageref{par:bsd}):
        \begin{enumerate}
            \item from the BSD dataset, extract the original image $B_0$ and give it to the NN;
            \item get the NN output $N_0$, which is a nearly segmented image;
            \item use the Eikonal algorithm to obtain the final superpixel segmented image $\bar N_f$, with superpixel labels, on which we compute the metrics.
        \end{enumerate}



\section{The model}
    \subsection{Network architecture}
        \subsubsection{Layers definitions}

            \paragraph{Dilated convolution}\label{par:dilated} We consider a layer $L=(L_j)_{j\in [\![1,w]\!]}$, $w$ being the number of feature maps $L_j$ of $L$. We also consider $K=(K_{i,j})_{i,j}$, each $K_{i,j}$ being a $3\times 3$ convolutional kernel. The dilated convolution operation of $K_{i,j}$ on $L_j$ is denoted by $L_j*_r K_{i,j}$, $r$ being the dilation parameter.

            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-simple.png}
                    \caption{\textit{A simple convolution ($r=1$)}}
                    \label{fig:conv-simple}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-dilated.png}
                    \caption{\textit{A dilated convolution ($r=2$)}}
                    \label{fig:conv-dilated}
                \end{subfigure}
                \caption{\textit{Illustration of two types of convolutions}}
            \end{figure}

            \noindent The output $C(x)$ of a pixel $x$ is:
            \begin{flalign*}
            C(x):& = (L_j*_r K_{i,j})(x) &\\
                 & = \sum_{a+rb=x}L_j(a)K_{i,j}(b) &\\
                 & = \sum_b L_j(x-rb)K_{i,j}(b) &
            \end{flalign*}
            and we recognize the simple convolution when $r=1$.\\
            A dilated convolution enables the network getting larger receptive fields while preserving the input resolution\footnote{ref ?}.


            \paragraph{Adaptative Batch Normalization (ABN)} Distinct images in the COCO dataset have very different color ranges; in order to maximize the network performances, we normalize the data. We define the \textit{adaptative normalization function} $\Psi$ as:
            $$\Psi(x)=a\ x+b\ BN(x),$$
            where $BN$ is the classic batch normalization\footnote{reference ?}, defined as:
            $$BN(x) = \frac{x-\mathrm{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta.$$
            As such, $\Psi$ combines identity mapping and batch normalization. $a$, $b$, $\gamma$ and $\beta$ are learned parameters\footnote{ref : \url{https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html}} by backpropagation. It allows the model to adapt to each dataset, choosing whether or not giving a big importance to the identity term and the normalization term.

            \paragraph{Leaky rectifier (LReLU)} In order to let our neural network model complex patterns in the data, we have to add a non-linear property to the model. It often is an activation function, such as a sigmoid or a tanh, but these activation functions are bounded and their gradient is very low on the edges. Because we are going to manipulate high scalar values, we have to use an unbounded activation function, such as ReLU, $\Phi(x)=\max(0,x)$ (Figure~\ref{fig:relu}). But the issue with ReLU is that all the negative values become zero immediately, which decreases the ability of our model to train from the data. Hence the implementation of a \textit{leaky rectifier}, LReLU (Figure~\ref{fig:lrelu}):
            $$\Phi(x)=\max(\alpha x,x)\mbox{, with } 0<\alpha<1.$$

            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.7\linewidth]{pics/act-relu.png}
                    \caption{\textit{ReLU}}
                    \label{fig:relu}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.7\linewidth]{pics/act-lrelu.png}
                    \caption{\textit{LReLU ($\alpha=0.2$)}}
                    \label{fig:lrelu}
                \end{subfigure}
                \caption{\textit{Illustration of two unbounded rectifiers}}
            \end{figure}

            \noindent By implementing a Leaky Rectifier, we are able to take into account the negative valued pixels.


        \newpage
        \subsubsection{Chen}
            \paragraph{Context Aggregation Network (CAN)}\footnote{reference}
            blabla sur le RGB en entrée, RGB en sortie I -> f(I)\\
            To do
            \begin{table}[!ht]
                \center
                \begin{tabular}{ccccccccccc}
                    \hline
                    input $I$ & $\longrightarrow$ & $L^1$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & $L^s$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & output ($L^d$)\\
                    $m\times n\times 3$ & & $m\times n\times w_1$ & & & & $m\times n\times w_s$ & & & & $m\times n\times 3$\\
                    \hline
                \end{tabular}
                \caption{\textit{Layers}}
            \end{table}

            \paragraph{Architecture of a block}
            Each block $L_s$ is made of 3 layers:
            \begin{enumerate}
                \item A \textit{dilated convolution}, with parameter $r_s=2^s$,
                \item An \textit{adaptative batch normalization},
                \item A \textit{leaky rectifier (ReLU)},
            \end{enumerate}
            so that the content of an intermediate layer $L^s$ can be computed from the content of the previous layer $L^{s-1}$:
            \begin{equation}
                L_i^s=\Phi\left(\Psi^s\left(b_i^s+\sum_jL_j^{s-1}*_{r_s}K^s_{i,j}\right)\right).
            \end{equation}
            where ... is ...

            and
            \begin{equation}
                L_j^{s-1}*_{r_s}K^s_{i,j}=\sum_{a+r_sb=x}L_j^{s-1}(a)K_{i,j}^s(b)
            \end{equation}
            because of \ref{par:dilated}, page~\pageref{par:dilated}.

            \begin{table}[!ht]
                \centering
                \begin{tabular}{|c|c|c|c|c|c|c|c|}
                    \hline
                    Layer & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
                    \hline \hline
                    Convolution & $3\times3$ & & & & & & \\
                    \hline
                    Dilation & 1 & & & & & & \\
                    \hline
                    Batch Normalization & Yes & & & & & & \\
                    \hline
                    LReLU & Yes & & & & & & \\
                    \hline
                \end{tabular}
                \caption{\textit{Chen}}
            \end{table}

        \subsubsection{UNet}
        To do
        \subsubsection{Chen + UNet}
        To do

    \subsection{Total Variation (TV) Loss}
        \subsubsection{MSE}
        To do
        $$L_{MSE}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2$$

        \subsubsection{TV}\footnote{ref}
        To do

        In such a search for well-segmented images, 2 criteria have to be fulfilled. The output needs to be as close as possible to the ground truth image; but we also need the segmented image to present a lot of zones where the color gradient $\nabla f(I)$ is equal to $0$. Thus, we want to implement a train loss function that could help us satisfy these two criteria.
        In order grant an improvement in the output image smoothness, we use the Total Variation (TV) loss, defined by:
        $$
        L_{TV}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2+\alpha_{TV}\frac{1}{N}\sum_{i=1}^N|(\nabla f(I))_i)|^2
        $$


\section{Experiments and results}
    \subsection{Implementation}
    The network was implemented with PyTorch\footnote{Repository can be found at \url{https://github.com/theodumont/superpixels-segmentation}.}. With a batch size of 32, the training of a model was quite long on a GPU, mainly due to the size of the training dataset and the complexity of our model. In further sections we discuss how the hyperparameters impact the model's performances --- metrics and temporal efficiency --- and we conduct experiments to find a good-performing architecture. We found that the following values worked well on the BSD dataset, so we take them as default in the next sections, and only change one or two hyperparameters at a time:
    \begin{table}[!ht]
        \centering
        \begin{tabular}{|c|c|c|c|c|}
            \hline
            batch\_size & epochs & $d$ & learning\_rate & $\alpha_{TV}$ \\
            \hline
            \hline
            32 & 80? & $7$ & $10^{-2}$, $10^{-3}$ & ? \\
            \hline
        \end{tabular}
        \caption{\textit{Hyperparameter values}}
    \end{table}


    \subsection{Hyperparameter tuning}
        \subsubsection{Learning rate}
            \begin{table}[!ht]
                \center
                \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    $lr_0$ & decay? & saturation? & $d$ & TV? \\
                    \hline \hline
                    $0.001$ & No & No & $7$ & No \\
                    \hline
                    $0.01$ & No & No & $7$ & No \\
                    \hline
                    $0.01$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                    \hline
                    $0.001$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                    \hline
                \end{tabular}
            \caption{Runs for learning rate tuning}
            \end{table}

            \begin{figure}[!ht]
                \begin{subfigure}{.33\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                    \caption{\textit{Graph}}
                \end{subfigure}
                \begin{subfigure}{.33\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                    \caption{\textit{Graph}}
                \end{subfigure}
                \begin{subfigure}{.33\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                    \caption{\textit{Graph}}
                \end{subfigure}
                \caption{\textit{Tuning of learning rate}}
            \end{figure}


            \paragraph{Constant value}
            entrainement (lr, alpha) -> courbes de loss, et loss qui sature (cluster) d'où changement de lr au cours des epochs
            \paragraph{Non constant value}
            to do
        $10^{-2}$ then $10^{-3}$ after 10 epochs.
        \subsubsection{Network size \textit{d}}
            - Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
            - Pas de régularisation TV

            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \center
                    \begin{tabular}{|c|c|c|c|c|c|}
                        \hline
                        $d$ & 4 & 5 & 6 & 7 & 8 \\
                        \hline \hline
                        loss$\times 10^3$ & $3.46$ & $3.18$ & $3.12$ & $3.13$ & ??? \\
                        \hline
                    \end{tabular}
                    \caption{\textit{Loss values on validation set}}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                    \caption{\textit{Graph}}
                \end{subfigure}
                \caption{\textit{Tuning of network size}}
            \end{figure}

            CONCLUSION des run 3 à 5: Il est préférable de laisser d=7. Entre d=6 et d=7, l'amélioration semble
            relativement faible.
            bon intermédiaire entre temps de calcul et performances

        \subsubsection{Number of epochs}
            nb epochs: on le sélectionne en prenant le minimum de la validation loss
        \subsubsection{TV regularization}
            attention changer valeur code
            PK MARCHE PAS ?
            - gradient fort sur outlines
            - comment faire ? découper images, COCO pas ouf pour ça, résolution basse
        \subsubsection{All the runs}
            a mettre en annexe, tableaux de tous les runs et graphes

    \subsection{Results on dataset}
    cf results/images
    BSD dataset, BSD fait a la main et pas a l'arrache comme COCO

        \paragraph{Results}
        Here are the previously defined metrics of some well-known superpixel segmentation algorithms.
        \begin{table}[!ht]
            \centering
            \begin{tabular}{|c|c|ccc|}
                \hline
                \multicolumn{2}{|c|}{Algorithm} & BR & UE & CO\\
                \hline
                \hline
                \multirow{3}{*}{image analysis} & EI [\footnote{eikonal}] & & & \\
                & EIT [\footnote{Eikonal with textures, metriques dans l'article}] & & & \\
                & WS [\footnote{watershed}] & & & \\
                \hline
                \multirow{2}{*}{neural networks} & ref [\footnote{SLIC?}] & 0.8995 & 0.0473 & 0.5409\\
                & Ours & 0.8781\footnote{Légèrement moins bon que ref, pas très grave parce que la compacité est pourrie du coup comme les contours oscillent ils intersectent plus de contours de l'image} & 0.0388 & 0.7682\\
                \hline
            \end{tabular}
            \caption{Comparisons of metrics on the BSD dataset for different
            superpixel segmentation algorithms}
        \end{table}

        \noindent We use the SLIC alorithm as a reference to evaluate the performances of our model.



\section{Conclusion/Discussion}
On a présenté un nouveau...\\
On a prouvé...\\
Il reste à faire...

relire tous les mails pour avoir toutes les infos sur performances etc

\section*{Special thanks}

\bibliographystyle{plain}
\bibliography{biblio}

\cite{shi_2000}
\cite{arbelaez_2011}
\cite{ren_2005}
\cite{felzenszwalb_2004}
\cite{chang_2019}
\cite{zitnick_2007}
\cite{fulkerson_2009}
\cite{stutz_2017}

\newpage

\section*{Appendices}

\begin{table}
    \centering
    \begin{tabular}{|c|}
        \hline
        cell \\
        \hline
    \end{tabular}
    \caption{All the runs}
\end{table}

\end{document}
