\documentclass{article}
\usepackage{preambule}

\begin{document}

\begin{center}
    \begin{Large}\textbf{IMAGE SEGMENTATION BY SUPERPIXELS}\end{Large}

    \vspace{1cm}
    \begin{large}\textbf{\underline{T.Dumont$^a$}, B.Figliuzzi$^b$}\end{large}

    \vspace{0.5cm}
    a. MINES ParisTech, theo.dumont@mines-paristech.fr\\
    b. MINES ParisTech CMM, bruno.figliuzzi@mines-paristech.fr
    \vspace{1cm}
\end{center}

\begin{center}
\noindent\textbf{Key-words: }\\
deep learning; convolutional neural networks; image segmentation\\
\ \\
\end{center}
\textbf{Abstract: }\\
In this paper, we present an algorithm based upon convolutional neural networks for generating superpixel partitions of images.\\
By combining an algorithm that generates superpixel partitions through the resolution of the Eikonal equation and ground truth segmentations from the Microsoft Common Objects in Context (COCO) dataset, we were able to generate training examples of superpixel partitions of the images of the dataset. These training examples arise in the form of RGB image where the color is averaged over each superpixel. A convolutional network architecture is then trained on these images. A superpixel algorithm is finally applied to the output of the network to construct the seeked partition.\\
The algorithm is evaluated on the Berkeley Segmentation Dataset 500. It yields results in terms of boundary adherence that are comparable to the ones obtained with state of the art algorithms including SLIC, while significantly improving on these algorithms in terms of compactness and undersegmentation.

\tableofcontents

\section*{Todo}
\begin{itemize}
    \item
\end{itemize}

\newpage
\section{Introduction}

    \subsection{Segmentation}
            The human eye can easily understand the content of an image, but making a computer interpret the disposition of its pixels is more complex. Segmenting an image is partitioning it into multiple sets of pixels. It transforms the image into something that is easier to analyze, and much more meaningful, assigning a label to every pixel in an image such that pixels with the same label share certain characteristics. It allows one to locate the objects on an image, pointing out their boundaries.

            \begin{figure}[!ht]
            \centering
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=0.9\linewidth]{pics/img_segm1.jpg}
                \caption{\textit{The original image}}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=0.9\linewidth]{pics/img_segm2.png}
                \caption{\textit{The segmented image}}
            \end{subfigure}
                \caption{\textit{A segmentation. To each segmented region of the image was affected the mean value of its pixels. Both images are from the COCO training dataset (REF?).}}
                \label{fig:segm}
            \end{figure}
            The applications of such a process are numerous: control of the an object outlines on a production line, face detection, medical imaging, pedestrian detection, video surveillance\ldots They justify our search for higher segmentation performances.

    \subsection{Superpixels}
        Superpixel algorithms are a class of techniques that partition an image into several small groups of pixels that share the same properties. Such a process highly reduces the number of characteristics of an image, as each superpixel is composed of hundreds of pixels, which leads to the amount of calculation being reduced for a further processing. In addition, as the pixels of a superpixel share similar attributes, they constitute regions of an image on which it is very relevant to compute features -- mean color, texture.

        \begin{figure}[!ht]
        \centering
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp1.png}
        \end{subfigure}
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp2.png}
        \end{subfigure}
        \begin{subfigure}{.3\linewidth}
            \centering
            \includegraphics[width=0.9\linewidth]{pics/img_spp3.png}
        \end{subfigure}
            \caption{\textit{A superpixel segmentation. From left to right: the original image, the original image with its calculated superpixels outlines and the resulting superpixel segmented image. Each superpixel has only one color, the mean color of the original image over the superpixel region.}}
            \label{fig:spp}
        \end{figure}

        Thus, superpixel algorithms are often considered as a pre-processing step in a number of applications. They can be used for depth estimation [depth], make the objects features more understandable in object classification, and improve performances in image segmentation [img-segm], reducing the complexity of finding the different regions of an image by grouping at first similar pixels.

        \subsubsection{A good superpixel segmentation}
            \paragraph{Metrics}
            Before even starting building one, we have to define what is a ``good'' superpixel segmentation algorithm. As it is shown in [stutz], it is a very ambiguous task as many relevant criteria exist. Several metrics, though, are commonly chosen in the literature to evaluate whether a superpixel algorithm gives good results or not.

            Let $G = \{G_i\}_i$ and $S = \{S_j\}_j$ be partitions of the same image $I : x_n \mapsto I(x_n)$, $1 \leq n \leq N$. $G$ is the ground truth segmented image\footnote{ground truth segmented or superpixel segmented?} and $S$ is the segmented image obtained from a superpixel algorithm.
            \begin{itemize}
                \item \textbf{Boundary Recall.} The first criterion we want the algorithm to respect is quite logically to detect most of the ground truth's outlines. Boundary recall measures the intersection between the dilated outlines of the segmented image and the ones of the ground truth image. As so, it indicates the proportion of real boundaries being detected, with a tolerance margin of a few pixels.\\
                If $\text{D}(G,\tilde{S})$ is the number of detected boundary pixels and $\text{UD}(G,\tilde{S})$ the number of undetected boundary pixels in the segmented image $S$, then the \textit{boundary recall} is:
                $$
                \mathrm{Rec}(G,S)=\frac{\mathrm{D}(G,\tilde{S})}{\mathrm{D}(G,\tilde{S})+\mathrm{UD}(G,\tilde{S})} \in [0,1]
                $$
                $\tilde{S}$ being the segmented image with its boundaries dilated. Please note that boundary recall does not measure the regularity of the boundaries at all. That means an algorithm can have a very high boundary recall while being very abrupt. This nourishes the need of a metric that quantifies the regularity of the boundaries.

                \item \textbf{Compactness.} In order to simplify the superpixel segmented image as much as possible, its superpixels need to be smooth and regular. We thus want to build a criterion that computes how close the area $A(S_j)$ of each superpixel $S_j$ is from a circle with same perimeter $P(S_j)$:
                $$
                \mathrm{Co}(G, S)=\frac{1}{N} \sum_{S_j}|S_j| \frac{4 \pi A(S_j)}{P(S_j)^2}
                $$

                \item \textbf{Undersegmentation Error.} Undersegmentation Error measures the ``leakage'' of the superpixels over the ground truth:
                $$
                \mathrm{UE}(G,S) = \text{formula}
                $$
            \end{itemize}

            \paragraph{SLIC} To do

        \subsubsection{Ambitions}
                \paragraph{Difficultés que l'on cherche à résoudre}
                \paragraph{Pas de vraie approche DL pour segmentation avec spps} Most of the aforementioned segmentation methods are based only on color information of pixels in the image. Humans use much more knowledge when performing image segmentation, but implementing this knowledge would cost considerable human engineering and computational time, and would require a huge domain knowledge database which does not currently exist. Trainable segmentation methods, such as neural network segmentation, overcome these issues by modeling the domain knowledge from a dataset of labeled pixels.
                \paragraph{Ambitions}
                améliorer les métriques









\section{Dataset generation}
    \subsection{COCO dataset}
        \subsubsection{The COCO dataset}
        COCO dataset\footnote{site de COCO}, nb of images, examples

        \subsubsection{Characteristics}
        \label{par:charac}
        In order to have a better understanding of the dataset, we

        \begin{figure}[!ht]
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{pics/train2017full.png}
                \caption{\textit{plot}}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=\linewidth]{pics/val2017full.png}
                \caption{\textit{plot}}
            \end{subfigure}
            \bigskip
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c||c|c|c|}
                    \hline
                     & Training & Height & Width \\
                    \hline
                    \hline
                    Images & Min & 51 & 59 \\
                    \hline
                    118~287 & Max & 640 & 640 \\
                    \hline
                \end{tabular}
                \caption{Tabular}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c||c|c|c|}
                    \hline
                     & Validation & Height & Width \\
                    \hline
                    \hline
                    Images & Min & 145 & 200 \\
                    \hline
                    5000 & Max & 640 & 640 \\
                    \hline
                \end{tabular}
                \caption{Tabular}
            \end{subfigure}
            \caption{\textit{Training and validation sets characterization}}
        \end{figure}

        d'où la nécessité de faire des transformations sur la dataset pour uniformiser les tailles en entrée du réseau.

    \subsection{Eikonal}
    \subsection{Global approach}
    en plus réutilisé derrière sur image qui sort du réseau\\
    faire un petit résumé
    \begin{figure}[!ht]
        \centering
        % \includegraphics[width=0.9\linewidth]{pics/img_segm3.png}
        \caption{\textit{Title}}
    \end{figure}









\section{The model}
    \subsection{Network architecture}
        \subsubsection{Layers definitions}

            \paragraph{Dilated convolution}\label{par:dilated} We consider a layer $L=(L_j)_{j\in [\![1,w]\!]}$, $w$ being the number of feature maps $L_j$ of $L$. We also consider $K=(K_{i,j})_{i,j}$, each $K_{i,j}$ being a $3\times 3$ convolutional kernel. The dilated convolution operation of $K_{i,j}$ on $L_j$ is denoted by $L_j*_r K_{i,j}$, $r$ being the dilation parameter.
            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-simple.png}
                    \caption{\textit{A simple convolution ($r=1$)}}
                    \label{fig:conv-simple}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.8\linewidth]{pics/conv-dilated.png}
                    \caption{\textit{A dilated convolution ($r=2$)}}
                    \label{fig:conv-dilated}
                \end{subfigure}
                \caption{\textit{Illustration of two types of convolutions}}
            \end{figure}

            \noindent The output $C(x)$ of a pixel $x$ is:
            \begin{flalign*}
            C(x):& = (L_j*_r K_{i,j})(x) &\\
                 & = \sum_{a+rb=x}L_j(a)K_{i,j}(b) &\\
                 & = \sum_b L_j(x-rb)K_{i,j}(b) &
            \end{flalign*}
            and we recognize the simple convolution when $r=1$.\\
            A dilated convolution enables the network getting larger receptive fields while preserving the input resolution\footnote{ref ?}


            \paragraph{Adaptative Batch Normalization (ABN)} As we have seen in (\ref{par:charac}), page \pageref{par:charac}, we need to normalize the data. We define the \textit{adaptative normalization function} $\Psi$ as:
            $$\Psi(x)=a\ x+b\ BN(x),$$
            where $BN$ is the classic batch normalization\footnote{reference ?}, defined as:
            $$BN(x) = \frac{x-\mathrm{E}[x]}{\sqrt{\mathrm{Var}[x]+\epsilon}}*\gamma+\beta.$$
            As such, $\Psi$ combines identity mapping and batch normalization. $a$, $b$, $\gamma$ and $\beta$ are learned parameters\footnote{ref : \url{https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html}} by backpropagation. It allows the model to adapt to each dataset, choosing whether or not giving a big importance to the identity term and the normalization term.

            \paragraph{Leaky rectifier (LReLU)} In order to let our neural network model complex patterns in the data, we have to add a non-linear property to the model. It often is an activation function, such as a sigmoid or a tanh (Figure \ref{fig:act-sigmoids}).

            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.9\linewidth]{pics/act-sigmoid.png}
                    \caption{\textit{Sigmoid}}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.9\linewidth]{pics/act-tanh.png}
                    \caption{\textit{tanh}}
                \end{subfigure}
                \caption{\textit{Illustration of two bounded rectifiers}}
                \label{fig:act-sigmoids}
            \end{figure}

            these activation functions are often used but they are bounded and their gradient is very low on the edges. Because we are going to manipulate high scalar values, we have to use an unbounded activation function, such as ReLU, $\Phi(x)=\max(0,x)$ (Figure \ref{fig:relu}). But the issue with ReLU is that all the negative values become zero immediately, which decreases the ability of our model to train from the data. Hence the implementation of a \textit{leaky rectifier}, LReLU (\ref{fig:lrelu}):
            $$\Phi(x)=\max(\alpha x,x)\mbox{, with } 0<\alpha<1.$$

            \begin{figure}[!ht]
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.7\linewidth]{pics/act-relu.png}
                    \caption{\textit{ReLU}}
                    \label{fig:relu}
                \end{subfigure}
                \begin{subfigure}{.49\linewidth}
                    \centering
                    \includegraphics[width=.7\linewidth]{pics/act-lrelu.png}
                    \caption{\textit{LReLU ($\alpha=0.2$)}}
                    \label{fig:lrelu}
                \end{subfigure}
                \caption{\textit{Illustration of two unbounded rectifiers}}
            \end{figure}

            By implementing a Leaky Rectifier, we are able to take into account the negative valued pixels.



        \subsubsection{Chen}
            \paragraph{Context Aggregation Network (CAN)}\footnote{reference}
            \begin{table}[!ht]
                \center
                \begin{tabular}{ccccccccccc}
                    \hline
                    input $I$ & $\longrightarrow$ & $L^1$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & $L^s$ & $\longrightarrow$ & $\cdots$ & $\longrightarrow$ & output ($L^d$)\\
                    $m\times n\times 3$ & & $m\times n\times w_1$ & & & & $m\times n\times w_s$ & & & & $m\times n\times 3$\\
                    \hline
                \end{tabular}
                \caption{\textit{Layers}}
            \end{table}
            blabla sur le RGB en entrée, RGB en sortie I -> f(I)
            \paragraph{Architecture of a block}
            Each block $L_s$ is made of 3 layers:
            \begin{enumerate}
                \item \textit{A dilated convolution}, $r_s=2^s$
                \item \textit{An adaptative batch normalization}
                \item \textit{A leaky rectifier (ReLU)}
            \end{enumerate}
            so that the content of an intermediate layer $L^s$ can be computed from the content of the previous layer $L^{s-1}$:
            \begin{equation}
                L_i^s=\Phi\left(\Psi^s\left(b_i^s+\sum_jL_j^{s-1}*_{r_s}K^s_{i,j}\right)\right).
            \end{equation}
            where ... is ...

            and
            \begin{equation}
                L_j^{s-1}*_{r_s}K^s_{i,j}=\sum_{a+r_sb=x}L_j^{s-1}(a)K_{i,j}^s(b)
            \end{equation}
            because of \ref{par:dilated}, page \pageref{par:dilated}.

            \begin{table}[!ht]
                \centering
                \begin{tabular}{|c|c|c|c|c|c|c|c|}
                    \hline
                    Layer & 1 & 2 & 3 & 4 & 5 & 6 & 7\\
                    \hline \hline
                    Convolution & $3\times3$ & & & & & & \\
                    \hline
                    Dilation & 1 & & & & & & \\
                    \hline
                    Batch Normalization & Yes & & & & & & \\
                    \hline
                    LReLU & Yes & & & & & & \\
                    \hline
                \end{tabular}
                \caption{\textit{Chen}}
            \end{table}

        \subsubsection{UNet}
        \subsubsection{Chen + UNet}

    \subsection{Total Variation (TV) Loss}
        \subsubsection{MSE}
        $$L_{MSE}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2$$

        \subsubsection{TV}\footnote{ref}
            \paragraph{Why} In such a search for well-segmented images, 2 criteria have to be fulfilled. The output needs to be as close as possible to the ground truth image; but we also need the segmented image to present a lot of zones where the color gradient $\nabla f(I)$ is equal to $0$. Thus, we want to implement a train loss function that could help us satisfy these two criteria.
            In order to do so, we use the Total Variation (TV) loss, defined below.

            \paragraph{Formula}
            $$L_{TV}=\frac{1}{N}\sum_{i=1}^N |\hat{f}(I)_i-f(I)_i|^2+\frac{1}{N}\sum_{i=1}^N|(\nabla f(I))_i)|^2$$
            granting an improvement in the output image smoothness.

    \subsection{Implementation}
    The network was implemented with PyTorch\footnote{Repository can be found at \url{https://github.com/theodumont/superpixels-segmentation}.} and we used GPU acceleration [...]
    (pytorch), se renseigner (section assez courante)
    GPU acceleraation
    code sur github









\section{Expérience et résultats}
    \subsection{Hyperparameters}
    petit bilan des valeurs choisies
    évolution des paramètres a et b ?\\
        \subsubsection{Learning rate}
        \begin{table}[!ht]
            \center
            \begin{tabular}{|c|c|c|c|c|}
                \hline
                $lr_0$ & decay? & saturation? & $d$ & TV? \\
                \hline \hline
                $0.001$ & No & No & $7$ & No \\
                \hline
                $0.01$ & No & No & $7$ & No \\
                \hline
                $0.01$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                \hline
                $0.001$ & $\times 0.5$ every 2 epochs & $10^{-4}$ & $7$ & No \\
                \hline
            \end{tabular}
        \caption{Runs for learning rate tuning}
        \end{table}

        \begin{figure}[!ht]
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \begin{subfigure}{.33\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \caption{\textit{Tuning of learning rate}}
        \end{figure}


        \paragraph{Constant value}
    entrainement (lr, alpha) -> courbes de loss, et loss qui sature (cluster) d'où changement de lr au cours des epochs
        \paragraph{Non constant value}
        \subsubsection{Network size $d$}
        - Learning rate initialisé à 0.01, divisé par 2 toutes les 2 époques, saturation à 1e-4
        - Pas de régularisation TV
        \begin{figure}[!ht]
            \begin{subfigure}{.49\linewidth}
                \center
                \begin{tabular}{|c|c|c|c|c|c|}
                    \hline
                    $d$ & 4 & 5 & 6 & 7 & 8 \\
                    \hline \hline
                    loss$\times 10^3$ & $3.46$ & $3.18$ & $3.12$ & $3.13$ & ??? \\
                    \hline
                \end{tabular}
                \caption{\textit{Loss values on validation set}}
            \end{subfigure}
            \begin{subfigure}{.49\linewidth}
                \centering
                \includegraphics[width=.8\linewidth]{pics/hpp-d.png}
                \caption{\textit{Graph}}
            \end{subfigure}
            \caption{\textit{Tuning of network size}}
        \end{figure}
        CONCLUSION des run 3 à 5: Il est préférable de laisser d=7. Entre d=6 et d=7, l'amélioration semble
        relativement faible.
        bon intermédiaire entre temps de calcul et performances

        \subsubsection{Number of epochs}
    nb epochs: on le sélectionne en prenant le minimum de la validation loss
        \subsubsection{TV regularization}

        \subsubsection{Runs}
        tableaux et graphes
    \subsection{Results on dataset}
    image originale -> CNN -> résultat du filtre dans eikonal -> superpixels sans couleurs + couleur moyenne pour chaque spp de l'image originale
    cf results/images


        \subsubsection{The dataset}
        BSD dataset, BSD fait a la main et pas a l'arrache comme COCO
        \subsubsection{Results}

            \paragraph{metrics}
            Here are the previously defined metrics of some well-known superpixel segmentation algorithms.
            \begin{table}[!ht]
                \centering
                \begin{tabular}{|c|c|ccc|}
                    \hline
                    \multicolumn{2}{|c|}{Algorithm} & BR & UE & CO\\
                    \hline
                    \hline
                    \multirow{3}{*}{image analysis} & EI [\footnote{eikonal}] & & & \\
                    & EIT [\footnote{Eikonal with textures, metriques dans l'article}] & & & \\
                    & WS [\footnote{watershed}] & & & \\
                    \hline
                    \multirow{2}{*}{neural networks} & ref [\footnote{SLIC?}] & 0.8995 & 0.0473 & 0.5409\\
                    & Ours & 0.8781\footnote{Légèrement moins bon que ref, pas très grave parce que la compacité est pourrie du coup comme les contours oscillent ils intersectent plus de contours de l'image} & 0.0388 & 0.7682\\
                    \hline
                \end{tabular}
                \caption{Comparisons of metrics on the BSD dataset for different
                superpixel segmentation algorithms}
            \end{table}

            \noindent We use the ?? alorithm as a reference to evaluate the performances of our model.
            pas très grave parce que la compacité est pourrie
            du coup comme les contours oscillent ils intersectent plus de contours de l'image








\section{Conclusion/Discussion}
On a présenté un nouveau...\\
On a prouvé...\\
Il reste à faire...

relire tous les mails pour avoir toutes les infos sur performances etc

\section*{Special thanks}


\include{biblio}

\end{document}
